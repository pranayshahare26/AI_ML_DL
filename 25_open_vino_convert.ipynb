{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Machine Learning\n",
    "\n",
    "## OpenVINO Text Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from openvino.runtime import Core\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"./utils\")\n",
    "from notebook_utils import download_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some Basic Parameters for housekeeping\n",
    "\n",
    "# set location of input files:\n",
    "inpDir = os.path.join('./')\n",
    "\n",
    "# set location of inputs for this module\n",
    "moduleDir = './'\n",
    "\n",
    "# set location of inputs for this module\n",
    "subDir = 'vino'\n",
    "\n",
    "# set location of output files\n",
    "outDir = os.path.join('./')\n",
    "\n",
    "# set location of model files\n",
    "modelDir = os.path.join('./')\n",
    "\n",
    "# define and set random state \n",
    "RANDOM_STATE = 24\n",
    "np.random.seed(RANDOM_STATE) # Set Random Seed for reproducible  results\n",
    "\n",
    "# parameters for Matplotlib\n",
    "params = {'legend.fontsize': 'large',\n",
    "          'figure.figsize': (15, 9),\n",
    "          'axes.labelsize': 'large',\n",
    "          'axes.titlesize':'x-large',\n",
    "          'xtick.labelsize':'x-large',\n",
    "          'ytick.labelsize':'large',\n",
    "          'savefig.dpi': 75,\n",
    "          'image.interpolation': 'none',\n",
    "          'savefig.bbox' : 'tight',\n",
    "          'lines.linewidth' : 1,\n",
    "          'legend.numpoints' : 1\n",
    "         }\n",
    "\n",
    "CMAP = plt.cm.brg\n",
    "plt.rcParams.update(params);\n",
    "plt.set_cmap(CMAP);\n",
    "\n",
    "plt.style.use('bmh')\n",
    "\n",
    "TEST_SIZE = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_dir = Path(\"./model\").expanduser() # expand the path as per the os and environment\n",
    "\n",
    "model_name = \"horizontal-text-detection-0001\"\n",
    "model_xml_name = f'{model_name}.xml'\n",
    "model_bin_name = f'{model_name}.bin'\n",
    "\n",
    "model_xml_path = base_model_dir / model_xml_name\n",
    "model_bin_path = base_model_dir / model_bin_name\n",
    "\n",
    "if not model_xml_path.exists():\n",
    "    model_xml_url = \"https://storage.openvinotoolkit.org/repositories/open_model_zoo/2022.3/models_bin/1/horizontal-text-detection-0001/FP32/horizontal-text-detection-0001.xml\"\n",
    "    model_bin_url = \"https://storage.openvinotoolkit.org/repositories/open_model_zoo/2022.3/models_bin/1/horizontal-text-detection-0001/FP32/horizontal-text-detection-0001.bin\"\n",
    "\n",
    "    download_file(model_xml_url, model_xml_name, base_model_dir)\n",
    "    download_file(model_bin_url, model_bin_name, base_model_dir)\n",
    "else:\n",
    "    print(f'{model_name} already downloaded to {base_model_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenVINO enables you to optimize a deep learning model from almost any framework and deploy it with best-in-class performance on a range of Intel processors and other hardware platforms.\n",
    "\n",
    "A typical workflow with OpenVINO is shown below.\n",
    "\n",
    "<img src='../../images/openVINO_fig2.png' width='600' alt=\"OpenVINO Workflow\"/>\n",
    "\n",
    "OpenVINO Runtime automatically optimizes deep learning pipelines using aggressive graph fusion, memory reuse, load balancing, and inferencing parallelism across CPU, GPU, VPU, and more. You can integrate and offload to accelerators additional operations for pre- and post-processing to reduce end-to-end latency and improve throughput."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boost your model’s speed even further with quantization and other state-of-the-art compression techniques available in OpenVINO’s Post-Training Optimization Tool and Neural Network Compression Framework. These techniques also reduce your model size and memory requirements, allowing it to be deployed on resource-constrained edge hardware.\n",
    "\n",
    "\n",
    "\n",
    "### Local Inferencing & Model Serving\n",
    "\n",
    "You can either link directly with OpenVINO Runtime to run inference locally or use OpenVINO Model Serving to serve model inference from separate server or within Kubernetes environment\n",
    "\n",
    "### Improved Application Portability\n",
    "\n",
    "Write an application once, deploy it anywhere, achieving maximum performance from hardware. Automatic device discovery allows for superior deployment flexibility. OpenVINO Runtime supports Linux, Windows and MacOS and provides Python, C++ and C API. Use your preferred language and OS.\n",
    "\n",
    "### Minimal External Dependencies\n",
    "\n",
    "Designed with minimal external dependencies reduces the application footprint, simplifying installation and dependency management. Popular package managers enable application dependencies to be easily installed and upgraded. Custom compilation for your specific model(s) further reduces final binary size.\n",
    "\n",
    "\n",
    "\n",
    "### Enhanced App Start-Up Time\n",
    "\n",
    "In applications where fast start-up is required, OpenVINO significantly reduces first-inference latency by using the CPU for initial inference and then switching to GPU or VPU once the model has been compiled and loaded to memory. Compiled models are cached to further improving start-up time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ie = Core()\n",
    "\n",
    "model = ie.read_model(model=model_xml_path)\n",
    "compiled_model = ie.compile_model(model=model, device_name=\"CPU\")\n",
    "\n",
    "input_layer_ir = compiled_model.input(0)\n",
    "output_layer_ir = compiled_model.output(\"boxes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load an Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text detection models expect an image in BGR format.\n",
    "image = cv2.imread(filename=os.path.join(inpDir,'basic_operations/IMG_1295.JPG'))\n",
    "\n",
    "# N,C,H,W = batch size, number of channels, height, width.\n",
    "N, C, H, W = input_layer_ir.shape\n",
    "\n",
    "# Resize the image to meet network expected input sizes.\n",
    "resized_image = cv2.resize(image, (W, H))\n",
    "\n",
    "# Reshape to the network input shape.\n",
    "input_image = np.expand_dims(resized_image.transpose(2, 0, 1), 0)\n",
    "\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an inference request.\n",
    "boxes = compiled_model([input_image])[output_layer_ir]\n",
    "\n",
    "# Remove zero only boxes.\n",
    "boxes = boxes[~np.all(boxes == 0, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confidence levels for various boxes\n",
    "for box in boxes:\n",
    "    print (box[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize\n",
    "For each detection, the description is in the [x_min, y_min, x_max, y_max, conf] format. The image passed here is in BGR format with changed width and height. To display it in colors expected by matplotlib, use cvtColor function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###----------------\n",
    "### Helper Function\n",
    "###----------------\n",
    "\n",
    "def convert_result_to_image(bgr_image, resized_image, boxes, threshold=0.2, conf_labels=True):\n",
    "    \n",
    "    # Define colors for boxes and descriptions.\n",
    "    colors = {\"red\": (255, 0, 0), \"green\": (0, 255, 0)}\n",
    "\n",
    "    # Fetch the image shapes to calculate a ratio.\n",
    "    (real_y, real_x), (resized_y, resized_x) = bgr_image.shape[:2], resized_image.shape[:2]\n",
    "    ratio_x, ratio_y = real_x / resized_x, real_y / resized_y\n",
    "\n",
    "    # Convert the base image from BGR to RGB format.\n",
    "    rgb_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Iterate through non-zero boxes.\n",
    "    for box in boxes:\n",
    "        # Pick a confidence factor from the last place in an array.\n",
    "        conf = box[-1]\n",
    "        if conf > threshold:\n",
    "            # Convert float to int and multiply corner position of each box by x and y ratio.\n",
    "            # If the bounding box is found at the top of the image,\n",
    "            # position the upper box bar little lower to make it visible on the image.\n",
    "            (x_min, y_min, x_max, y_max) = [\n",
    "                int(max(corner_position * ratio_y, 10)) if idx % 2\n",
    "                else int(corner_position * ratio_x)\n",
    "                for idx, corner_position in enumerate(box[:-1])\n",
    "            ]\n",
    "\n",
    "            # Draw a box based on the position, parameters in rectangle function are: image, start_point, end_point, color, thickness.\n",
    "            rgb_image = cv2.rectangle(rgb_image, (x_min, y_min), (x_max, y_max), colors[\"green\"], 3)\n",
    "\n",
    "            # Add text to the image based on position and confidence.\n",
    "            # Parameters in text function are: image, text, bottom-left_corner_textfield, font, font_scale, color, thickness, line_type.\n",
    "            if conf_labels:\n",
    "                rgb_image = cv2.putText(\n",
    "                    rgb_image,\n",
    "                    f\"{conf:.2f}\",\n",
    "                    (x_min, y_min - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.8,\n",
    "                    colors[\"red\"],\n",
    "                    2,\n",
    "                    cv2.LINE_AA,\n",
    "                )\n",
    "\n",
    "    return rgb_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(convert_result_to_image(image, resized_image, boxes, conf_labels=True));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
