{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Machine Learning\n",
    "\n",
    "## Convert a PyTorch Model to ONNX and OpenVINO IR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the PyTorch model is exported in ONNX format and then converted to OpenVINO IR. Then the respective ONNX and OpenVINO IR models are loaded into OpenVINO Runtime to show model predictions. In this tutorial, we will use LR-ASPP model with MobileNetV3 backbone.\n",
    "\n",
    "According to the paper, Searching for MobileNetV3, LR-ASPP or Lite Reduced Atrous Spatial Pyramid Pooling has a lightweight and efficient segmentation decoder architecture. The diagram below illustrates the model architecture:\n",
    "<img src='../../images/OpenVINO_MobiNet.png' style = 'width:400px;' alt=\"movinet\"/>\n",
    "\n",
    "The model is pre-trained on the MS COCO dataset. Instead of training on all 80 classes, the segmentation model has been trained on 20 classes from the PASCAL VOC dataset: **background, aeroplane, bicycle, bird, boat, bottle, bus, car, cat, chair, cow, dining table, dog, horse, motorbike, person, potted plant, sheep, sofa, train, tvmonitor**\n",
    "\n",
    "[Model Details](https://pytorch.org/vision/main/models/lraspp.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from IPython.display import Markdown, display\n",
    "from torchvision.models.segmentation import lraspp_mobilenet_v3_large, LRASPP_MobileNet_V3_Large_Weights\n",
    "from openvino.runtime import Core\n",
    "\n",
    "sys.path.append(\"./utils\")\n",
    "from notebook_utils import segmentation_map_to_image, viz_result_image, SegmentationMap, Label, download_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n",
    "Set a name for the model, then define width and height of the image that will be used by the network during inference. According to the input transforms function, the model is pre-trained on images with a height of 520 and width of 780."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some Basic Parameters for housekeeping\n",
    "\n",
    "# set location of input files:\n",
    "inpDir = os.path.join('./')\n",
    "\n",
    "# set location of inputs for this module\n",
    "moduleDir = './'\n",
    "\n",
    "# set location of inputs for this module\n",
    "subDir = 'vino'\n",
    "\n",
    "# set location of output files\n",
    "outDir = os.path.join('./')\n",
    "\n",
    "# set location of model files\n",
    "modelDir = os.path.join('./')\n",
    "\n",
    "# define and set random state \n",
    "RANDOM_STATE = 24\n",
    "np.random.seed(RANDOM_STATE) # Set Random Seed for reproducible  results\n",
    "\n",
    "# parameters for Matplotlib\n",
    "params = {'legend.fontsize': 'large',\n",
    "          'figure.figsize': (15, 9),\n",
    "          'axes.labelsize': 'large',\n",
    "          'axes.titlesize':'x-large',\n",
    "          'xtick.labelsize':'x-large',\n",
    "          'ytick.labelsize':'large',\n",
    "          'savefig.dpi': 75,\n",
    "          'image.interpolation': 'none',\n",
    "          'savefig.bbox' : 'tight',\n",
    "          'lines.linewidth' : 1,\n",
    "          'legend.numpoints' : 1\n",
    "         }\n",
    "\n",
    "CMAP = plt.cm.brg\n",
    "plt.rcParams.update(params);\n",
    "plt.set_cmap(CMAP);\n",
    "\n",
    "plt.style.use('bmh')\n",
    "\n",
    "IMAGE_WIDTH = 780\n",
    "IMAGE_HEIGHT = 520\n",
    "\n",
    "#DIRECTORY_NAME = \"model\"\n",
    "BASE_MODEL_NAME = os.path.join(modelDir, subDir, 'lraspp_mobilenet_v3_large')\n",
    "weights_path = Path(BASE_MODEL_NAME + \".pt\")\n",
    "\n",
    "# Paths where ONNX and OpenVINO IR models will be stored.\n",
    "onnx_path = weights_path.with_suffix('.onnx')\n",
    "if not onnx_path.parent.exists():\n",
    "    onnx_path.parent.mkdir()\n",
    "ir_path = onnx_path.with_suffix(\".xml\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model\n",
    "Generally, PyTorch models represent an instance of torch.nn.Module class, initialized by a state dictionary with model weights. Typical steps for getting a pre-trained model: 1. Create instance of model class 2. Load checkpoint state dict, which contains pre-trained model weights 3. Turn model to evaluation for switching some operations to inference mode\n",
    "\n",
    "The torchvision module provides a ready to use set of functions for model class initialization. We will use torchvision.models.segmentation.lraspp_mobilenet_v3_large. You can directly pass pre-trained model weights to the model initialization function using weights enum LRASPP_MobileNet_V3_Large_Weights.COCO_WITH_VOC_LABELS_V1. However, for demonstration purposes, we will create it separately. Download the pre-trained weights and load the model. This may take some time if you have not downloaded the model before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Downloading the LRASPP MobileNetV3 model (if it has not been downloaded already)...\")\n",
    "download_file(LRASPP_MobileNet_V3_Large_Weights.COCO_WITH_VOC_LABELS_V1.url, \n",
    "              filename=weights_path.name, \n",
    "              directory=weights_path.parent)\n",
    "# create model object\n",
    "model = lraspp_mobilenet_v3_large()\n",
    "# read state dict, use map_location argument to avoid a situation where weights are saved in cuda (which may not be unavailable on the system)\n",
    "state_dict = torch.load(weights_path, map_location='cpu')\n",
    "# load state dict to model\n",
    "model.load_state_dict(state_dict)\n",
    "# switch model from training to inference mode\n",
    "model.eval()\n",
    "print(\"Loaded PyTorch LRASPP MobileNetV3 model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONNX Model Conversion\n",
    "### Convert PyTorch model to ONNX\n",
    "\n",
    "OpenVINO supports PyTorch models that are exported in ONNX format. We will use the torch.onnx.export function to obtain the ONNX model, you can learn more about this feature in the PyTorch documentation. We need to provide a model object, example input for model tracing and path where the model will be saved. When providing example input, it is not necessary to use real data, dummy input data with specified shape is sufficient. Optionally, we can provide a target onnx opset for conversion and/or other parameters specified in documentation (e.g. input and output names or dynamic shapes).\n",
    "\n",
    "Sometimes a warning will be shown, but in most cases it is harmless, so let us just filter it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    if not onnx_path.exists():\n",
    "        dummy_input = torch.randn(1, 3, IMAGE_HEIGHT, IMAGE_WIDTH)\n",
    "        torch.onnx.export(\n",
    "            model,\n",
    "            dummy_input,\n",
    "            onnx_path,\n",
    "        )\n",
    "        print(f\"ONNX model exported to {onnx_path}.\")\n",
    "    else:\n",
    "        print(f\"ONNX model {onnx_path} already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert ONNX Model to OpenVINO IR Format\n",
    "\n",
    "Use Model Optimizer to convert the ONNX model to OpenVINO IR with FP16 precision. The models are saved inside the current directory. For more information about Model Optimizer, see the Model Optimizer Developer Guide.\n",
    "\n",
    "Executing this command may take a while. There may be some errors or warnings in the output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the command for Model Optimizer.\n",
    "mo_command = f'mo --input_model \"{onnx_path}\" --compress_to_fp16 --output_dir \"{ir_path.parent}\"'\n",
    "                 \n",
    "mo_command = \" \".join(mo_command.split())\n",
    "\n",
    "print(\"Model Optimizer command to convert the ONNX model to OpenVINO:\")\n",
    "display(Markdown(f\"`{mo_command}`\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ir_path.exists():\n",
    "    print(\"Exporting ONNX model to IR... This may take a few minutes.\")\n",
    "    mo_result = %sx $mo_command\n",
    "    print(\"\\n\".join(mo_result))\n",
    "else:\n",
    "    print(f\"IR model {ir_path} already exists.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Results\n",
    "\n",
    "Confirm that the segmentation results look as expected by comparing model predictions on the ONNX, OpenVINO IR and PyTorch models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(image: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Normalize the image to the given mean and standard deviation\n",
    "    for models.\n",
    "    \"\"\"\n",
    "    image = image.astype(np.float32)\n",
    "    mean = (0.485, 0.456, 0.406)\n",
    "    std = (0.229, 0.224, 0.225)\n",
    "    image /= 255.0\n",
    "    image -= mean\n",
    "    image /= std\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load an Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text detection models expect an image in BGR format.\n",
    "image_filename = os.path.join(inpDir,'basic_operations/bright_image_cropped.JPG')\n",
    "\n",
    "image = cv2.cvtColor(cv2.imread(image_filename), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "resized_image = cv2.resize(image, (IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "normalized_image = normalize(resized_image)\n",
    "\n",
    "# Convert the resized images to network input shape.\n",
    "input_image = np.expand_dims(np.transpose(resized_image, (2, 0, 1)), 0)\n",
    "normalized_input_image = np.expand_dims(np.transpose(normalized_image, (2, 0, 1)), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the OpenVINO IR Network and Run Inference on the ONNX model\n",
    "\n",
    "OpenVINO Runtime can load ONNX models directly. First, load the ONNX model, do inference and show the results. Then, load the model that was converted to OpenVINO Intermediate Representation (OpenVINO IR) with Model Optimizer and do inference on that model, and show the results on an image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the network to OpenVINO Runtime.\n",
    "ie = Core()\n",
    "model_onnx = ie.read_model(model=onnx_path)\n",
    "compiled_model_onnx = ie.compile_model(model=model_onnx, device_name=\"CPU\")\n",
    "\n",
    "output_layer_onnx = compiled_model_onnx.output(0)\n",
    "\n",
    "# Run inference on the input image.\n",
    "res_onnx = compiled_model_onnx([normalized_input_image])[output_layer_onnx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_labels = [\n",
    "    Label(index=0, color=(0, 0, 0), name=\"background\"),\n",
    "    Label(index=1, color=(128, 0, 0), name=\"aeroplane\"),\n",
    "    Label(index=2, color=(0, 128, 0), name=\"bicycle\"),\n",
    "    Label(index=3, color=(128, 128, 0), name=\"bird\"),\n",
    "    Label(index=4, color=(0, 0, 128), name=\"boat\"),\n",
    "    Label(index=5, color=(128, 0, 128), name=\"bottle\"),\n",
    "    Label(index=6, color=(0, 128, 128), name=\"bus\"),\n",
    "    Label(index=7, color=(128, 128, 128), name=\"car\"),\n",
    "    Label(index=8, color=(64, 0, 0), name=\"cat\"),\n",
    "    Label(index=9, color=(192, 0, 0), name=\"chair\"),\n",
    "    Label(index=10, color=(64, 128, 0), name=\"cow\"),\n",
    "    Label(index=11, color=(192, 128, 0), name=\"dining table\"),\n",
    "    Label(index=12, color=(64, 0, 128), name=\"dog\"),\n",
    "    Label(index=13, color=(192, 0, 128), name=\"horse\"),\n",
    "    Label(index=14, color=(64, 128, 128), name=\"motorbike\"),\n",
    "    Label(index=15, color=(192, 128, 128), name=\"person\"),\n",
    "    Label(index=16, color=(0, 64, 0), name=\"potted plant\"),\n",
    "    Label(index=17, color=(128, 64, 0), name=\"sheep\"),\n",
    "    Label(index=18, color=(0, 192, 0), name=\"sofa\"),\n",
    "    Label(index=19, color=(128, 192, 0), name=\"train\"),\n",
    "    Label(index=20, color=(0, 64, 128), name=\"tv monitor\")\n",
    "]\n",
    "VOCLabels = SegmentationMap(voc_labels)\n",
    "\n",
    "# Convert the network result to a segmentation map and display the result.\n",
    "result_mask_onnx = np.squeeze(np.argmax(res_onnx, axis=1)).astype(np.uint8)\n",
    "viz_result_image(\n",
    "    image,\n",
    "    segmentation_map_to_image(result_mask_onnx, VOCLabels.get_colormap()),\n",
    "    resize=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenVINO IR Model in OpenVINO Runtime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the network in OpenVINO Runtime.\n",
    "ie = Core()\n",
    "model_ir = ie.read_model(model=ir_path)\n",
    "compiled_model_ir = ie.compile_model(model=model_ir, device_name=\"CPU\")\n",
    "\n",
    "# Get input and output layers.\n",
    "output_layer_ir = compiled_model_ir.output(0)\n",
    "\n",
    "# Run inference on the input image.\n",
    "res_ir = compiled_model_ir([normalized_input_image])[output_layer_ir]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_mask_ir = np.squeeze(np.argmax(res_ir, axis=1)).astype(np.uint8)\n",
    "viz_result_image(\n",
    "    image,\n",
    "    segmentation_map_to_image(result=result_mask_ir, colormap=VOCLabels.get_colormap()),\n",
    "    resize=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
