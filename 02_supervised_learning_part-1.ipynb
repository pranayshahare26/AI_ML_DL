{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning, Artificial Intelligence and Deep Learning\n",
    "\n",
    "## - Classification -\n",
    "\n",
    "###  Supervised Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###------------------\n",
    "### Import statements\n",
    "###------------------\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'./'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.path.join('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###---------------------------------------\n",
    "### Some Basic Parameters for housekeeping\n",
    "###--------------------------------------\n",
    "\n",
    "# set location of input files:\n",
    "inpDir = os.path.join('./')\n",
    "\n",
    "# set location of inputs for this module\n",
    "moduleDir = './'\n",
    "\n",
    "# set location of output files\n",
    "outDir = os.path.join('./')\n",
    "\n",
    "# define and set random state \n",
    "RANDOM_STATE = 24\n",
    "np.random.seed(RANDOM_STATE) # Set Random Seed for reproducible  results\n",
    "\n",
    "# parameters for Matplotlib\n",
    "params = {'legend.fontsize': 'large',\n",
    "          'figure.figsize': (15, 9),\n",
    "          'axes.labelsize': 'large',\n",
    "          'axes.titlesize':'x-large',\n",
    "          'xtick.labelsize':'x-large',\n",
    "          'ytick.labelsize':'large',\n",
    "          'savefig.dpi': 75,\n",
    "          'image.interpolation': 'none',\n",
    "          'savefig.bbox' : 'tight',\n",
    "          'lines.linewidth' : 1,\n",
    "          'legend.numpoints' : 1\n",
    "         }\n",
    "\n",
    "CMAP = plt.cm.rainbow\n",
    "plt.rcParams.update(params);\n",
    "plt.set_cmap(CMAP);\n",
    "\n",
    "\n",
    "TEST_SIZE = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "P(X|Y) = $\\frac{P(Y|X)P(Y)}{P(X)}$\n",
    "\n",
    "<div>\n",
    "    <img src = '../../images/iris.png' width='500 px' align ='left'>\n",
    "    <img src = '../../images/iris_petal_sepal.png' width='200 px',  align ='left'>\n",
    "    <img src='../../images/petal_sepal.png' style = 'width:150px;' alt=\"iris images\" align=\"right\"/>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### DataSet\n",
    "1. Number of Instances: 150 (50 in each of three classes)\n",
    "2. Number of Attributes: 4 numeric, predictive attributes and the class\n",
    "3. Attribute Information:\n",
    "\n",
    "   a. sepal length in cm\n",
    "   \n",
    "   b. sepal width in cm\n",
    "   \n",
    "   c. petal length in cm\n",
    "   \n",
    "   d. petal width in cm\n",
    "   \n",
    "   e. class:\n",
    "      - Iris Setosa\n",
    "      - Iris Versicolour\n",
    "      - Iris Virginica\n",
    "      \n",
    "      \n",
    "4. Missing Attribute Values: None\n",
    "5. Class Distribution: 33.3% for each of 3 classes.\n",
    "\n",
    "\n",
    "## Loading from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Path where csv file is\n",
    "csvPath = os.path.join(inpDir, moduleDir, 'iris.csv')\n",
    "csvPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column Names\n",
    "iris_cols = ['sepal length', 'sepal width', 'petal length', 'petal width', 'target']\n",
    "\n",
    "# Read as a DataFrame\n",
    "iris_df = pd.read_csv(csvPath, \n",
    "                      skiprows= 1, header = None, names = iris_cols)\n",
    "iris_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "type(iris_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "iris_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "iris_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "\n",
    "Extract features and labels from the dataframe.\n",
    "\n",
    "**Remember:** We can use only numerical values in the features and labels. Incidentally, Iris dataset contains numerical values only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features\n",
    "X = iris_df.drop('target', axis = 1).to_numpy()\n",
    "\n",
    "# labels\n",
    "y = iris_df['target'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "type(X), type(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for col in iris_df:\n",
    "    print (col, iris_df[col].unique())\n",
    "    print ('-'*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good idea to see what is in the features\n",
    "\n",
    "plt.figure(figsize = (15,8))\n",
    "\n",
    "for ii in range (X.shape[1]):\n",
    "    \n",
    "        plt.subplot(2, 2, ii+1 ) # subplot for each of the column\n",
    "        \n",
    "        n, bins, patches = plt.hist( X [ :, ii], 30)\n",
    "        \n",
    "        plt.title('{}'.format(iris_cols[ii])) # put column name on the plot\n",
    "        \n",
    "        plt.axis('tight')\n",
    "        \n",
    "        #plt.legend(loc = 'best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier\n",
    "\n",
    "$Initialization \\rightarrow fit \\rightarrow predict$\n",
    "\n",
    "> from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "> clf = GaussianNB()\n",
    "\n",
    "> clf.fit(features, labels)\n",
    "\n",
    "> y_pred = clf.predict(features)\n",
    "\n",
    "> target_names = ['setosa','versicolor','virginica']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Classifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Instantiate\n",
    "clf = GaussianNB()\n",
    "\n",
    "# Fit\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make Predictions\n",
    "y_pred = clf.predict(X)\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#ax.scatter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (12,6))\n",
    "\n",
    "target_nms = [ 'Setosa','Versicolour','Virginica']\n",
    "\n",
    "X_plot = iris_df['sepal length'].to_numpy() \n",
    "\n",
    "Y_plot = iris_df['sepal width'].to_numpy()\n",
    "\n",
    "ax = fig.add_subplot()\n",
    "ax.scatter(X_plot, Y_plot, c = y, s = 200)\n",
    "ax.scatter(X_plot, Y_plot, c = y_pred, edgecolors='k', marker = '*', s = 100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use `sklearn` built-in function to calculate accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines\n",
    "\n",
    "The line that maximizes the minimum margin.\n",
    "\n",
    "The model class of 'hyper-planes with a margin of m' has a low VC dimension if m is big.\n",
    "This maximum-margin separator is determined by a subset of the datapoints.\n",
    "Datapoints in this subset  are called “support vectors”.\n",
    "It will be useful computationally if only a small fraction of the datapoints are support vectors, because we use the support vectors to decide which side of the separator a test case is on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "clf = SVC(gamma='auto')\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (12,6))\n",
    "\n",
    "target_nms = [ 'Setosa','Versicolour','Virginica']\n",
    "X_plot = iris_df['sepal length'].to_numpy() \n",
    "\n",
    "Y_plot = iris_df['sepal width'].to_numpy()\n",
    "\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "ax.scatter(X_plot, Y_plot, c = y, s = 200)\n",
    "\n",
    "ax.scatter(X_plot, Y_plot, c = y_pred, edgecolors='k', marker = '*', s = 100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SKLEARN Data sets\n",
    "### Toy\n",
    "- Toy datasets : sklearn.datasets.load_\n",
    "- Real world datasets : sklearn.datasets.fetch_\n",
    "- Generate: sklearn.datasets.make_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import Datasets\n",
    "from sklearn import datasets\n",
    "\n",
    "# Use `make_blob` create data with 50 rows  2 columns and 2 classes\n",
    "data = datasets.make_blobs(n_samples=50,  # How many data points\n",
    "                           n_features=2, # number of features\n",
    "                           centers=2,  # how many classes\n",
    "                           random_state=RANDOM_STATE, \n",
    "                           cluster_std=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X, y = data\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c = y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf = SVC(kernel = 'rbf')\n",
    "clf.fit(X, y)\n",
    "y_pred  = clf.predict(X)\n",
    "\n",
    "accuracy_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf.support_vectors_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Make predictions using Support Vector Machines\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC(kernel='linear')\n",
    "\n",
    "X, y = make_blobs(n_samples=50, centers=2,\n",
    "                  random_state=RANDOM_STATE, cluster_std=0.60)\n",
    "\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c = y)\n",
    "plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], \n",
    "            s = 100, \n",
    "            edgecolors='r',\n",
    "            facecolor = 'none',\n",
    "            alpha = 0.8);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Tuning\n",
    "Support vector Machines have a number of tunable parameters. Frequently used ones are :\n",
    "- Kernel\n",
    "- C\n",
    "- Gamma\n",
    "\n",
    "So, far we have used only kernel = `linear` which uses \"one-against-one\" approach (Knerr et al., 1990). Other kernels are `poly`, `rbf(Radial Basis Function)`, `sigmoid`, `precomputed` or a callable. If none is given, `rbf` will be used.\n",
    "\n",
    "## C Value\n",
    "Let's try to plot using three values of **C ; 0.001, 1, 1000**.\n",
    "\n",
    "## Kernel\n",
    "We will be trying three kernels:\n",
    "- Linear gives linear decision frontiers. It is the most computationally efficient approach and the one that requires the least amount of data.\n",
    "- rbf uses 'radial basis functions' centered at each support vector to assemble a decision frontier. The size of the RBFs, that ultimately controls the smoothness of the decision frontier. RBFs are the most flexible approach, but also the one that will require the largest amount of data.\n",
    "- poly gives decision frontiers that are polynomial. The order of this polynomial is given by the 'order' argument.\n",
    "\n",
    "## Gamma\n",
    "We will be using three values of **Gamma ; 0.001, 1 and auto**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for c in [0.001, 1, 1000]:\n",
    "    for k in ['linear', 'rbf', 'poly']:\n",
    "        for g in [0.001, 1, 'auto']:\n",
    "            clf = SVC(kernel = k, C=c, gamma=g)\n",
    "    \n",
    "            clf.fit(X, y)\n",
    "            y_pred  = clf.predict(X)\n",
    "\n",
    "            print (f'C: {c}; kernel: {k}; gamma: {g}|: Acc:{accuracy_score(y, y_pred): .2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unique thing about SVM is that **only the support vectors matter**; that is, if you moved any of the other points without letting them cross the decision boundaries, they would have no effect on the classification results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets load some more data\n",
    "\n",
    "> digits = sklearn.datasets.load_digits()\n",
    "\n",
    "> print(digits.keys())\n",
    "\n",
    ">X_train, X_test, y_train, y_test = train_test_split(\n",
    "    digits.data, \n",
    "    digits.target, \n",
    "    stratify=digits.target, \n",
    "    random_state=RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()\n",
    "\n",
    "print(digits.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(digits.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "digits.data.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(digits.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "digits.target.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = digits.data\n",
    "y = digits.target\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= TEST_SIZE,\n",
    "                                                    stratify=y, \n",
    "                                                    random_state=RANDOM_STATE)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "digits.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,3, figsize = (15,6))\n",
    "i = 0\n",
    "for k in ['linear', 'rbf', 'poly']:\n",
    "    clf = SVC(kernel = k)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred  = clf.predict(X_test)\n",
    "    print (f'Kernel: {k}: - Acc:{accuracy_score(y_test, y_pred): .4}', end = ' ')\n",
    "    print (f' - F1:{f1_score(y_test, y_pred, average=\"weighted\"):0.4f}')\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    #print (cm)\n",
    "    #print ('-'*50)\n",
    "    ax = axes[i]\n",
    "    ax.set_title(k)\n",
    "    i = i+1\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, \n",
    "                                  display_labels=digits.target_names\n",
    "                                 )\n",
    "    disp.plot(ax =ax, cmap = plt.cm.Blues, colorbar=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear models for classification\n",
    "\n",
    "Linear models are also extensively used for classification. \n",
    "\n",
    "A prediction is made using the following formula:\n",
    "\n",
    "$\\hat{y} = w_{0}*x_{0} + w_{1}*x_{1} + ... + w_{p}*x_{p} + b > 0$\n",
    "\n",
    "Now instead of just returning the weighted sum of the features, we threshold the predicted value at zero. If the function is smaller than zero, we predict the class –1; if it is larger than zero, we predict the class +1.\n",
    "\n",
    "The two most common linear classification algorithms are **logistic regression** and linear **support vector machines\n",
    "(linear SVMs)**.\n",
    "\n",
    "Despite its name, logistic regression is a classification algorithm and not a regression algorithm, and it should not be confused with linear Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X, y = datasets.make_blobs(n_samples=50, centers=2, random_state=RANDOM_STATE, cluster_std=0.7)\n",
    "\n",
    "plt.scatter(X[:,0], X[:,1], c = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X,y)\n",
    "\n",
    "y_pred = clf.predict(X)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "ax.scatter(X[:, 0], X[:,1], c = y, s = 200)\n",
    "\n",
    "ax.scatter(X[:, 0], X[:,1], c = y_pred, edgecolors='k', marker = '*', s = 100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "Decision trees at their root are extremely intuitive.  They encode a series of binary choices in a process that parallels how a person might classify things themselves, but using an information criterion to decide which question is most fruitful at each step.  Its like playing 20-questions game.\n",
    "\n",
    "For example, if you wanted to create a guide to identifying an animal found in nature, you might ask the following series of questions:\n",
    "\n",
    "- Is the animal bigger or smaller than a meter long?\n",
    "    + *bigger*: does the animal have horns?\n",
    "        - *yes*: are the horns longer than ten centimeters?\n",
    "        - *no*: is the animal wearing a collar\n",
    "    + *smaller*: does the animal have two or four legs?\n",
    "        - *two*: does the animal have wings?\n",
    "        - *four*: does the animal have a bushy tail?\n",
    "\n",
    "and so on.  This binary splitting of questions is the essence of a decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "#Instantiate the Graph\n",
    "decision_tree = graphviz.Digraph(node_attr={'shape': 'box'},\n",
    "                                 edge_attr={'labeldistance': \"10.5\"},\n",
    "                                 format=\"png\")\n",
    "\n",
    "###---------------\n",
    "### add some nodes\n",
    "###---------------\n",
    "decision_tree.node(\"0\", \"Has feathers?\")\n",
    "\n",
    "decision_tree.node(\"1\", \"Can fly?\")\n",
    "\n",
    "decision_tree.node(\"2\", \"Has fins?\")\n",
    "\n",
    "decision_tree.node(\"3\", \"Hawk\")\n",
    "\n",
    "decision_tree.node(\"4\", \"Penguin\")\n",
    "\n",
    "decision_tree.node(\"5\", \"Dolphin\")\n",
    "\n",
    "decision_tree.node(\"6\", \"Bear\")\n",
    "\n",
    "###---------------\n",
    "### add some edges\n",
    "###---------------\n",
    "decision_tree.edge(\"0\", \"1\", label=\"True\")\n",
    "\n",
    "decision_tree.edge(\"0\", \"2\", label=\"False\")\n",
    "\n",
    "decision_tree.edge(\"1\", \"3\", label=\"True\")\n",
    "\n",
    "decision_tree.edge(\"1\", \"4\", label=\"False\")\n",
    "\n",
    "decision_tree.edge(\"2\", \"5\", label=\"True\")\n",
    "\n",
    "decision_tree.edge(\"2\", \"6\", label=\"False\")\n",
    "\n",
    "decision_tree.render(\"images/decision_tree\")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.imshow(mpimg.imread(\"images/decision_tree.png\"))\n",
    "\n",
    "ax.set_axis_off()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = datasets.make_blobs(n_samples=300, centers=4,\n",
    "                  random_state=RANDOM_STATE, cluster_std=0.60)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, s=80, edgecolors='black')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Trees have a number of tunable parameters. \n",
    "\n",
    "class sklearn.tree.DecisionTreeClassifier(**criterion**='gini', splitter='best', **max_depth**=None, **min_samples_split**=2, **min_samples_leaf**=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, class_weight=None, presort=False)\n",
    "\n",
    "Parameters of interest are marked in bold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=10)\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.load_breast_cancer()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, \n",
    "                                                    test_size= TEST_SIZE,\n",
    "                                                    stratify=data.target, \n",
    "                                                    random_state=RANDOM_STATE)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(random_state=RANDOM_STATE)\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf.score(X_train, y_train), clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=2, random_state=RANDOM_STATE)\n",
    "\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy on train set: {:.3f}\".format(tree.score(X_train, y_train)))\n",
    "\n",
    "print(\"Accuracy on test set : {:.3f}\".format(tree.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "export_graphviz(tree, out_file=\"tree.dot\", class_names=[\"malignant\", \"benign\"],\n",
    "                feature_names=data.feature_names, impurity=False, filled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import graphviz\n",
    "\n",
    "with open(\"tree.dot\") as f:\n",
    "    dot_graph = f.read()\n",
    "img = graphviz.Source(dot_graph)\n",
    "! dot -Tpng tree.dot -o tree.png\n",
    "fig, ax = plt.subplots()\n",
    "ax.axis('off')\n",
    "ax.imshow(mpimg.imread(\"tree.png\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controlling complexity of decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ram_prices = pd.read_csv(os.path.join(inpDir, moduleDir,\"ram_price.csv\"))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.semilogy(ram_prices.date, ram_prices.price)\n",
    "ax.set_xlabel(\"Year\")\n",
    "ax.set_ylabel(\"Price in $/Mbyte\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# use historical data to forecast prices after the year 2000\n",
    "data_train = ram_prices[ram_prices.date < 2000]\n",
    "data_test = ram_prices[ram_prices.date >= 2000]\n",
    "\n",
    "# predict prices based on date\n",
    "X_train = data_train.date.to_numpy()[:, np.newaxis]\n",
    "\n",
    "# we use a log-transform to get a simpler relationship of data to target\n",
    "y_train = np.log(data_train.price)\n",
    "\n",
    "tree = DecisionTreeRegressor(max_depth=3).fit(X_train, y_train)\n",
    "\n",
    "linear_reg = LinearRegression().fit(X_train, y_train)\n",
    "\n",
    "# predict on all data\n",
    "X_all = ram_prices.date.to_numpy()[:, np.newaxis]\n",
    "\n",
    "pred_tree = tree.predict(X_all)\n",
    "\n",
    "pred_lr = linear_reg.predict(X_all)\n",
    "\n",
    "# undo log-transform\n",
    "price_tree = np.exp(pred_tree)\n",
    "\n",
    "price_lr = np.exp(pred_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.semilogy(data_train.date, data_train.price, label=\"Training data\")\n",
    "\n",
    "ax.semilogy(data_test.date, data_test.price, label=\"Test data\")\n",
    "\n",
    "ax.semilogy(ram_prices.date, price_tree, label=\"Tree prediction\")\n",
    "\n",
    "ax.semilogy(ram_prices.date, price_lr, label=\"Linear prediction\")\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "One problem with decision trees is that they can end up **over-fitting** the data. They are such flexible models that, given a large depth, they can quickly memorize the inputs, which doesn't generalize well to previously unseen data. One way to get around this is to use many slightly different decision trees in concert. This is known as **Random Forests**, and is one of the more common techniques of **ensemble learning** (i.e. combining the results from several estimators.\n",
    "\n",
    "<img src = '../../images/ensamble.png' alt='Ensamble and Random forest' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's difficult to see in this example, but for more complicated data, random forests can be a very powerful technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying Digits\n",
    "\n",
    "Lets work on digits dataset... its good time to check efficacy of RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = datasets.load_digits()\n",
    "\n",
    "digits.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = digits.data\n",
    "\n",
    "y = digits.target\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "images = digits.images\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the figure\n",
    "fig = plt.figure(figsize=(12, 12))  # figure size in inches\n",
    "\n",
    "# plot the digits: each image is 8x8 pixels\n",
    "for i in range(64):\n",
    "    ax = fig.add_subplot(8, 8, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(digits.images[i], cmap=plt.cm.binary, interpolation='nearest')\n",
    "    \n",
    "    # label the image with the target value\n",
    "    ax.text(0, 7, str(digits.target[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=5)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                       display_labels=digits.target_names)\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (9,9))\n",
    "\n",
    "disp.plot(ax = ax, cmap=plt.cm.Blues, colorbar=False);\n",
    "\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier #### Change\n",
    "\n",
    "fig, axes = plt.subplots(1,3, figsize = (15,6))\n",
    "i = 0\n",
    "for k in [20, 50, 100]: ### Change\n",
    "    \n",
    "    clf = RandomForestClassifier(max_depth= k) ### Change\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred  = clf.predict(X_test)\n",
    "    print (f'Depth: {k}: - Acc:{accuracy_score(y_test, y_pred): .4}', end = ' ')\n",
    "    print (f' - F1:{f1_score(y_test, y_pred, average=\"weighted\"):0.4f}')\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    #print (cm)\n",
    "    #print ('-'*50)\n",
    "    ax = axes[i]\n",
    "    ax.set_title(f'Depth:{k}')\n",
    "    i = i+1\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, \n",
    "                                  display_labels=digits.target_names\n",
    "                                 )\n",
    "    disp.plot(ax =ax, cmap = plt.cm.Blues, colorbar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# random forest results\n",
    "fig, axes = plt.subplots(1,3, figsize = (15,8))\n",
    "\n",
    "for i, max_depth in enumerate([3, 5, 10]):\n",
    "    \n",
    "    clf = RandomForestClassifier(max_depth=max_depth).fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    print(\"RF: max_depth = {:3d}\".format(max_depth), end = ' | ')\n",
    "    print(\"F1 Score : {: .4f}\".format(f1_score(y_test, y_pred, average='weighted')))\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                       display_labels=digits.target_names)\n",
    "\n",
    "    disp.plot(ax = axes[i], cmap=plt.cm.Blues, colorbar=False);\n",
    "    axes[i].set_title(\"RF: max_depth = {0}\".format(max_depth))\n",
    "    axes[i].grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Neighbors Classifier\n",
    "\n",
    "Loading Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "\n",
    "iris.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, n_features = iris.data.shape\n",
    "print (n_samples, n_features)\n",
    "print( iris.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# get the label Names as well\n",
    "target_names = iris.target_names\n",
    "\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=1)\n",
    "clf.fit(X, y)\n",
    "y_pred = clf.predict(X)\n",
    "print(np.all(y == y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "from sklearn.metrics import euclidean_distances\n",
    "\n",
    "def plot_knn_classification(X_train, y_train, X_test, n_neighbors=1):\n",
    "    \n",
    "    dist = euclidean_distances(X_train, X_test)\n",
    "    closest = np.argsort(dist, axis=0)\n",
    "\n",
    "    for x, neighbors in zip(X_test, closest.T):\n",
    "        for neighbor in neighbors[:n_neighbors]:\n",
    "            plt.arrow(x[0], x[1], X[neighbor, 0] - x[0],\n",
    "                      X[neighbor, 1] - x[1], head_width=0, fc='k', ec='k')\n",
    "\n",
    "    clf = KNeighborsClassifier(n_neighbors=n_neighbors).fit(X_train, y_train)\n",
    "    plt.scatter(X_test[:,0], X_test[:,1], s=30, c=clf.predict(X_test), marker = '*', cmap=CMAP)\n",
    "    plt.scatter(X_train[:,0], X_train[:,1], s=30, c=y_train, marker = 'o', cmap=CMAP)\n",
    "\n",
    "    #test_points = discrete_scatter(X_test[:, 0], X_test[:, 1], clf.predict(X_test), markers=\"*\")\n",
    "    #training_points = discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "    plt.legend(training_points + test_points, [\"training class 0\", \"training class 1\",\n",
    "                                               \"test pred 0\", \"test pred 1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A better representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "#  Split the data in training and test sets to measure performance of the model.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE )\n",
    "\n",
    "print (X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# scale our data\n",
    "sc = StandardScaler()\n",
    "\n",
    "###--------------------------------\n",
    "### always:\n",
    "###   fit_transform train data and\n",
    "###   transform only test data\n",
    "###-------------------------------\n",
    "\n",
    "X_s_train = sc.fit_transform(X_train)\n",
    "X_s_test = sc.transform(X_test)    \n",
    "\n",
    "lda = LinearDiscriminantAnalysis(n_components=2)\n",
    "X_train2 = lda.fit_transform(X_s_train, y_train)\n",
    "X_test2 = lda.transform(X_s_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = 1\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=n_neighbors).fit(X_train2, y_train)\n",
    "y_pred = clf.predict(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = euclidean_distances(X_train2, X_test2)\n",
    "closest = np.argsort(dist, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets Plot the data\n",
    "plt.scatter(X_train2[:,0], X_train2[:,1], s=20, c=y_train, edgecolors='k')\n",
    "plt.scatter(X_test2[:,0], X_test2[:,1], s=100, marker ='*', \n",
    "            c=y_pred, edgecolors='k')\n",
    "for x, neighbors in zip(X_test2, closest.T):\n",
    "    for neighbor in neighbors[:n_neighbors]:\n",
    "        plt.arrow(x[0], x[1], X_train2[neighbor, 0] - x[0],\n",
    "                  X_train2[neighbor, 1] - x[1], head_width=0, fc='k', ec='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = 3\n",
    "\n",
    "clf = KNeighborsClassifier(n_neighbors=n_neighbors).fit(X_train2, y_train)\n",
    "y_pred = clf.predict(X_test2)\n",
    "\n",
    "# Lets Plot the data\n",
    "plt.scatter(X_train2[:,0], X_train2[:,1], s=20, c=y_train,  edgecolors='k')\n",
    "plt.scatter(X_test2[:,0], X_test2[:,1], s=100, marker ='*', \n",
    "            c=y_pred, edgecolors='k')\n",
    "for x, neighbors in zip(X_test2, closest.T):\n",
    "    for neighbor in neighbors[:n_neighbors]:\n",
    "        plt.arrow(x[0], x[1], X_train2[neighbor, 0] - x[0],\n",
    "                  X_train2[neighbor, 1] - x[1], head_width=0, fc='k', ec='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_plot_decision_boundary(clf, X_train2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_accuracy = []\n",
    "test_accuracy = []\n",
    "\n",
    "# try n_neighbors from 1 to 10\n",
    "\n",
    "for n_neighbors in range(1, 11):\n",
    "    # build the model\n",
    "    clf = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    clf.fit(X_train, y_train)\n",
    "    # record training set accuracy\n",
    "    training_accuracy.append(clf.score(X_train, y_train))\n",
    "    # record generalization accuracy\n",
    "    test_accuracy.append(clf.score(X_test, y_test))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(range(1, 11), training_accuracy, label=\"Train Acc\")\n",
    "ax.plot(range(1, 11), test_accuracy, label=\"Test Acc\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_xlabel(\"# neighbors\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD Classifier\n",
    "\n",
    "Linear classifiers (SVM, logistic regression, etc.) with SGD training.\n",
    "\n",
    "This estimator implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). SGD allows minibatch (online/out-of-core) learning via the partial_fit method. For best results using the default learning rate schedule, the data should have zero mean and unit variance.\n",
    "\n",
    "This implementation works with data represented as dense or sparse arrays of floating point values for the features. The model it fits can be controlled with the loss parameter; by default, it fits a linear support vector machine (SVM).\n",
    "\n",
    "The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# we create 50 separable points\n",
    "X, Y = make_blobs(n_samples=50, centers=2, random_state=0, cluster_std=0.60)\n",
    "\n",
    "# fit the model\n",
    "clf = SGDClassifier(loss=\"hinge\", alpha=0.01, max_iter=200)\n",
    "\n",
    "clf.fit(X, Y)\n",
    "\n",
    "# plot the line, the points, and the nearest vectors to the plane\n",
    "xx = np.linspace(-1, 5, 10)\n",
    "yy = np.linspace(-1, 5, 10)\n",
    "\n",
    "X1, X2 = np.meshgrid(xx, yy)\n",
    "Z = np.empty(X1.shape)\n",
    "for (i, j), val in np.ndenumerate(X1):\n",
    "    x1 = val\n",
    "    x2 = X2[i, j]\n",
    "    p = clf.decision_function([[x1, x2]])\n",
    "    Z[i, j] = p[0]\n",
    "levels = [-1.0, 0.0, 1.0]\n",
    "\n",
    "linestyles = [\"dashed\", \"solid\", \"dashed\"]\n",
    "\n",
    "colors = \"k\"\n",
    "\n",
    "plt.contour(X1, X2, Z, levels, colors=colors, linestyles=linestyles)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired, edgecolor=\"black\", s=50)\n",
    "\n",
    "plt.axis(\"tight\")\n",
    "\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
